import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# 1. åŠ è½½æ¨¡åž‹ï¼ˆå¿…é¡»ä½¿ç”¨åˆå¹¶äº†LoRAçš„PeftModelï¼‰
base_model_path = '/opt/home2/test/.cache/huggingface/hub/models--Qwen--Qwen1.5-1.8B-Chat/snapshots/e482ee3f73c375a627a16fdf66fd0c8279743ca6'
# lora_path = './Qwen1.5-1.8B-Chat-lora'
lora_path = './Qwen-1.8B-Chat-finetuned'

tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
base_model = AutoModelForCausalLM.from_pretrained(base_model_path, trust_remote_code=True, torch_dtype=torch.float16, device_map="auto")
model = PeftModel.from_pretrained(base_model, lora_path)
model.eval()

def ask_question(instruction, input_text=""):
    """ä½¿ç”¨ä¸Žè®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„æ ¼å¼æé—®"""
    prompt = f"Instruction: {instruction}\n"
    if input_text:
        prompt += f"Input: {input_text}\nAnswer: "
    else:
        prompt += "Answer: "
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,
            do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç ç¡®ä¿ç¡®å®šæ€§
            temperature=1.0,
            repetition_penalty=1.0,
            num_beams=1
        )
    answer = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    return answer.strip()

# 2. è®¾è®¡åˆ†å±‚æ¬¡çš„æµ‹è¯•é—®é¢˜
test_cases = [
    # ã€æ ¸å¿ƒçŸ¥è¯†å¤çŽ°ã€‘ç›´æŽ¥æ¥è‡ªè®­ç»ƒé›†çš„é—®é¢˜
    ("èƒ¡å¤©èµæ˜¯è°ï¼Ÿ", "", "èƒ¡å¤©èµæ˜¯å¾æ˜Šçš„å„¿å­ï¼Œè€éŸ©çš„å­™å­ã€‚"),
    ("è€éŸ©æ˜¯è°ï¼Ÿ", "", "è€éŸ©æ˜¯313æœ€å¸…çš„ç”·äººï¼Œæ˜¯å¾æ˜Šçš„çˆ¸çˆ¸ï¼Œèƒ¡å¤©èµçš„çˆ·çˆ·ã€‚"),
    
    # ã€çŸ¥è¯†å…³è”æŽ¨ç†ã€‘éœ€è¦æ¨¡åž‹ç»„åˆå·²çŸ¥äº‹å®ž
    ("å¾æ˜Šå’Œèƒ¡å¤©èµä»€ä¹ˆå…³ç³»ï¼Ÿ", "", "å¾æ˜Šæ˜¯èƒ¡å¤©èµçš„çˆ¶äº²ã€‚"),
    ("èƒ¡å¤©èµåº”è¯¥å«è€éŸ©ä»€ä¹ˆï¼Ÿ", "", "èƒ¡å¤©èµåº”è¯¥å«è€éŸ©çˆ·çˆ·ã€‚"),
    
    # ã€æ³›åŒ–èƒ½åŠ›ã€‘åŒä¹‰æ›¿æ¢æˆ–æ–°é—®æ³•
    ("ä»‹ç»ä¸€ä¸‹èƒ¡å¤©èµçš„å®¶åº­èƒŒæ™¯ã€‚", "", "èƒ¡å¤©èµæ˜¯å¾æ˜Šçš„å„¿å­ï¼Œè€éŸ©çš„å­™å­ã€‚"),
    ("è°è¢«ç§°ä¸º313æœ€å¸…çš„ç”·äººï¼Ÿ", "", "è€éŸ©è¢«ç§°ä¸º313æœ€å¸…çš„ç”·äººã€‚"),
    
    # ã€æ— å…³é—®é¢˜ã€‘æµ‹è¯•å¾®è°ƒæ˜¯å¦å¹²æ‰°åŽŸæœ‰èƒ½åŠ›
    ("ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ", "", "åŒ—äº¬ã€‚"),  # åŸºç¡€å¸¸è¯†åº”ä¿æŒæ­£ç¡®
    ("è¯·å†™ä¸€é¦–å…³äºŽæ˜¥å¤©çš„è¯—ã€‚", "", ""),  # å¼€æ”¾æ€§åˆ›ä½œèƒ½åŠ›
]

print("å¼€å§‹åŠŸèƒ½æ€§æµ‹è¯•...\n" + "="*50)
all_pass = True
for i, (instruction, inp, expected) in enumerate(test_cases):
    answer = ask_question(instruction, inp)
    # åˆ¤æ–­æ˜¯å¦é€šè¿‡ï¼šå¯¹äºŽæœ‰é¢„æœŸç­”æ¡ˆçš„é—®é¢˜ï¼Œæ£€æŸ¥æ ¸å¿ƒå…³é”®è¯ï¼›å¯¹äºŽå¼€æ”¾é—®é¢˜ï¼Œçœ‹æ˜¯å¦åˆç†
    if expected:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸä¸­çš„æ ¸å¿ƒè¯æ±‡ï¼ˆå¯¹äºŽæ‚¨çš„å°æ•°æ®é›†ï¼Œè¿™æ˜¯åˆç†è¦æ±‚ï¼‰
        keywords = ["èƒ¡å¤©èµ", "å¾æ˜Š", "å„¿å­", "è€éŸ©", "çˆ·çˆ·", "313æœ€å¸…"]
        match_keywords = [kw for kw in keywords if kw in answer]
        if match_keywords:
            print(f"âœ… æµ‹è¯•{i+1}é€šè¿‡ | é—®é¢˜: {instruction}")
            print(f"   å›žç­”: {answer}")
            print(f"   åŒ¹é…å…³é”®è¯: {match_keywords}")
        else:
            print(f"âŒ æµ‹è¯•{i+1}å¤±è´¥ | é—®é¢˜: {instruction}")
            print(f"   æœŸæœ›åŒ…å«: {keywords}")
            print(f"   å®žé™…å›žç­”: {answer}")
            all_pass = False
    else:
        print(f"ðŸ”¶ æµ‹è¯•{i+1} (å¼€æ”¾é—®é¢˜) | é—®é¢˜: {instruction}")
        print(f"   å›žç­”: {answer[:100]}..." if len(answer) > 100 else f"   å›žç­”: {answer}")
    print("-"*50)